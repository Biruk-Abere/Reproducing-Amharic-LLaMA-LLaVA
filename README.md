<div align="center">

  <h2> <span style="font-size:12px">Reproducing Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource </span> </h2> 

  <p align="center">
  <img src="https://img.shields.io/github/package-json/v/sultan99/react-on-lambda.svg" alt="GitHub package.json version">
  <img alt="GitHub" src="https://img.shields.io/github/license/easybase/easybase-react">
  <img alt="npm bundle size" src="https://img.shields.io/bundlephobia/min/easybase-react">
  <img alt="npm" src="https://img.shields.io/npm/v/easybase-react">
</p>
</div>
<br>

**Abstract:-** Large language models (LLMs) like GPT, Llama and others have demonstrated unprecedented capabilities in language understanding and generation. These models excel at a variety of tasks, but primarily in languages that are well-represented in their training sets â€” such as English. However, they struggle when it comes to low-resource languages like Amharic, the most widely spoken language in Ethiopia with approximately 60 million speakers worldwide. In the paper called Amharic Llama the authors tried to explore taining LLaMA-2 to understand and generated Amharic text. Now we are attempting to reproduce the paper using different Architectures, Language Models and settings.